{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: changed calculation of expected p values according to Anna's code\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "# Read a .txt or .csv file containing SNPs and P values to be plotted\n",
    "# Function checks delimiters automatically\n",
    "# Parameter: filename - .txt or .csv file containing information needed\n",
    "# Return: pandas dataframe of the file\n",
    "def __read_file(filename):\n",
    "    fh = open(filename, 'r')\n",
    "    line = fh.readline()\n",
    "    if line=='': return None # Empty file, return None\n",
    "    \n",
    "    tmp = line.strip().split(',')\n",
    "\n",
    "    # Create an empty dataframe, read data into df later\n",
    "    df = pd.DataFrame()\n",
    "    # Check delimiter type, \",\" or \" \"\n",
    "    # One of the read_csv() call should work\n",
    "    if len(tmp) == 1:\n",
    "        try: df = pd.read_csv(filename, sep='\\t')\n",
    "        except pd.errors.ParserError: pass\n",
    "#              print('Try 2 catched')     \n",
    "        \n",
    "        try: df = pd.read_csv(filename, delim_whitespace=True)\n",
    "        except pd.errors.ParserError: pass\n",
    "#             print('Try 2 catched')\n",
    "#         File format maybe wrong if both read_csv failed\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(filename)\n",
    "    \n",
    "    # File only has a line of tile, return None  \n",
    "    if len(df)==0: return None \n",
    "    \n",
    "    return(df)\n",
    "\n",
    "\n",
    "# Return -log10 transformed observed and expected p vlaues of a dataset(dataframe)\n",
    "# Saved some old code for future reference\n",
    "def __log_obsv_and_expt(df, column_title):\n",
    "    scale = 0.5/np.log(10)\n",
    "    Pobsv = df.loc[:, column_title]  # Observed p values\n",
    "    Pobsv = np.sort(Pobsv.dropna().values)\n",
    "    \n",
    "    # Calculate expected p\n",
    "    # A method from qqman package. Refer to ppooints function in R.\n",
    "    # The results are slightly differetn from Anna's\n",
    "#     if len(Pobsv)>10:\n",
    "#         Pexpt = (np.arange(1, len(Pobsv)+1)-0.5) / (len(Pobsv)+1-1)\n",
    "#     else:\n",
    "#         Pexpt = (np.arange(1, len(Pobsv)+1)-3.0/8) / (len(Pobsv)+1-2*3.0/8)\n",
    "        \n",
    "        # From Anna's code, she used this one to calculate chi-squared test statistics, but not for logP qq plot\n",
    "#         Pexpt = chi2.ppf(np.arange(1, len(Pobsv) + 1, 1) / (len(Pobsv) + 1), df=1)\n",
    "\n",
    "    ln_Pexpt = 2*np.cumsum(1/np.arange(len(Pobsv), 0, -1)) # -2*ln(Pexpt), from Anna's code, don't understand why\n",
    "    logPobsv = -np.log10(Pobsv)\n",
    "    logPexpt = ln_Pexpt * scale # Convert from -2ln(x) to -log(x). from Anna's code\n",
    "    \n",
    "    return(Pobsv, np.sort(logPobsv), logPexpt)\n",
    "\n",
    "\n",
    "# Calculate and return lambda (inflation)\n",
    "# This is different from Anna's code, which used mean(obsvd)/mean(expctd)?\n",
    "def __get_inflation__(observed):\n",
    "    obsv_median = np.median(observed)\n",
    "    Chi = chi2.ppf(1.0 - obsv_median, 1) # 1 refers to freedom of 1\n",
    "    lmbd = Chi / chi2.ppf(0.5, 1)\n",
    "    return lmbd\n",
    "\n",
    "\n",
    "# Plot confidence interval\n",
    "def __plot_ci(ax, ci, logPexpt):\n",
    "    n = len(logPexpt)\n",
    "    df = 2 # Degree of freedom, from Anna's code (Why 2?)\n",
    "    # From Anna's code. Scale is used to convert 2ln(x) to log10(x), but I don't understand it\n",
    "    scale = 0.5/np.log(10)\n",
    "\n",
    "    a = list(range(1, n+1))\n",
    "    b = list(range(n, 0, -1))\n",
    "\n",
    "    clower = beta.ppf(q=(1-ci)/2, a = a, b = b)\n",
    "    cupper = beta.ppf(q=1-(1-ci)/2, a = a, b = b)\n",
    "    lower_bond = chi2.ppf(q=clower, df=df)\n",
    "    upper_bond = chi2.ppf(q=cupper, df=df)\n",
    "\n",
    "    plt.fill_between(x=logPexpt, y1=lower_bond*scale, y2=upper_bond*scale,\n",
    "                     alpha=0.2, color='k', linewidth=0, zorder=0) # Plot at the bottom\n",
    "\n",
    "    \n",
    "# The tricky part for me is to query haploreg database thorugh their website.\n",
    "# Inspecct the element of the website to find out names of each variable\n",
    "# Recommand using Chrome, go to developer mode, refresh the website under Network tab\n",
    "# This function is intended to query less than 1000 SNPs at a time, otherwise HaploReg may not be happy\n",
    "# Return: a set of rsIDs of accociated SNPs (and the queried SNP)\n",
    "\n",
    "# Parameters needed to send to haploreg website, and some explaination\n",
    "# input_snps (query): input SNPs in a string, each SNP is separated by ','\n",
    "#        SNP number per query is set to be 1000, otherwise it takes too long and haploreg may refuse to do it.\n",
    "#        15000 SNPs did not work for test run.\n",
    "# input_pos: a single region as chrN:start-end (Not a list!), to find a SNP by position.\n",
    "#            Only the first position will be queried if user supplied multiple positions.\n",
    "# gwas_id: the dropdown list to choose GWAS study, when no file or query SNP(s) is provided\n",
    "# r2_threshold (ldThresh): r^2 threshold, default is 0.2 in this code\n",
    "# ldPop: 1000G Phase 1 population for LD calculation. Other options includes AFR, AMR and ASN.\n",
    "# epi: Source for epigenomes\n",
    "# cons: Mammalian conservation algorithm. 'siphy'=SiPhy-omega, 'gerp'=GERP, or 'both'\n",
    "# genetypes: Show position relative to\n",
    "# output: set output result type to 'text' for python code to process\n",
    "def query_haploreg(input_snp='',\n",
    "                   input_pos='',\n",
    "                   r2_threshold=0.2,\n",
    "                   ldPop='EUR',\n",
    "                   epi='vanilla',\n",
    "                   cons='siphy',\n",
    "                   genetypes='gencode'):\n",
    "\n",
    "    if input_snp != '':\n",
    "        if input_pos != '':\n",
    "            print('HaploReg query issue: rsID and position can not be queried together')\n",
    "            return([])\n",
    "        input_content = input_snp       \n",
    "    elif input_pos !=\"\":\n",
    "        if input_pos[:3].lower() != 'chr':\n",
    "            print('HaploReg query issue: wrong format of position. Need to be chrN:start-end')\n",
    "            return([])       \n",
    "        # convert input_pos to lowercase, since only 'chr' will work\n",
    "        input_content = input_pos.lower()\n",
    "    \n",
    "    params_library = {'query':input_content,\n",
    "                      'gwas_id':0,\n",
    "                      'ldThresh': r2_threshold,\n",
    "                      'ldPop': ldPop,\n",
    "                      'epi': epi,\n",
    "                      'cons': cons,\n",
    "                      'genetypes': genetypes,\n",
    "                      'output':'text'}\n",
    "    \n",
    "    # parameters passed to the website, needs to be parsed and in binary\n",
    "    params = urllib.parse.urlencode(params_library).encode(\"utf-8\")\n",
    "    # url of HaploReg4.1\n",
    "    url = 'https://pubs.broadinstitute.org/mammals/haploreg/haploreg.php'\n",
    "    \n",
    "    # Keep console running\n",
    "    print('-- Waiting for HaploReg to return results...')\n",
    "    \n",
    "    # Query with parameters\n",
    "    query = urllib.request.urlopen(url, params)    \n",
    "    content = query.read().decode(\"utf-8\")\n",
    "    \n",
    "    # Find accociated SNPs in the content returned from HaploReg\n",
    "    matches = re.findall('rs[0-9]+', content)\n",
    "\n",
    "    # Return unique ones of the query\n",
    "    return(list(set(matches)))\n",
    "\n",
    "\n",
    "# This function returns known SNPs (and associated SNPs) based on user input file\n",
    "# Ideally know SNPs should not be more than 1500, otherwise it is too much on HaploReg v4.1\n",
    "# Parameters:\n",
    "#  - filename: a file contains known SNPs, provided by user\n",
    "#  - r2_threshold: LD threshold, defualt is 0.2\n",
    "#  - SNPs_column_title: Column title of SNPs in that file\n",
    "# Return: a set of associated SNPs pulled from HaploReg and known SNPs provided by user\n",
    "def __known_SNPs(filename, ldPop, r2_threshold=0.2, known_SNPs_column_title='SNP'):\n",
    "    # Keep console running\n",
    "    print('Getting associated SNPs of known SNPs from HaploReg')\n",
    "    \n",
    "    known_SNPs_df = __read_file(filename) # Dataframe of known SNPs\n",
    "    \n",
    "    # Split into smaller lists to query HaploReg (100 SNPs each query)\n",
    "    count = 0\n",
    "    split_point = 99\n",
    "    associated_and_known_SNPs = [] # Store queried results from HaploReg in a list\n",
    "    if len(known_SNPs_df)>100:\n",
    "        for i in range(0,len(known_SNPs_df), 100):\n",
    "            # Get SNPs to be queried, join them by ',' and query together\n",
    "            known_SNPs = ','.join(known_SNPs_df.loc[:, known_SNPs_column_title].values[i:split_point])\n",
    "            associated_and_known_SNPs = associated_and_known_SNPs + query_haploreg(input_snp=known_SNPs,\n",
    "                                                                                   r2_threshold=r2_threshold,\n",
    "                                                                                   ldPop=ldPop)\n",
    "            \n",
    "            print('  -- Number of SNPs queried: ' +  str((split_point+1)))\n",
    "            \n",
    "            if split_point+100 < len(known_SNPs_df):\n",
    "                split_point = split_point+100\n",
    "            else: split_point = len(known_SNPs_df)\n",
    "\n",
    "    # Remove duplicates of queried results and return\n",
    "    return(set(associated_and_known_SNPs))\n",
    "\n",
    "\n",
    "# This function removes known SNPs and save novel SNPs in a .csv file\n",
    "# Parameters:\n",
    "#  - original_df: a dataframe containing raw data of SNPs and p values\n",
    "#  - known_SNPs: a list of SNPs found in other GWAS, that are associated with traits of interest\n",
    "#  - known_SNPs_column_title: Column title of SNPs in that file\n",
    "# Return: a datafram of novel SNPs and p values\n",
    "def __remove_known_SNPs(original_df, known_SNPs,all_SNPs_column_title='SNP',\n",
    "                        novel_filename='novel_SNPs.csv'):\n",
    "    # Keep console running\n",
    "    print('Removing known SNPs by LD...')\n",
    "    \n",
    "    # Join the original dataframe and konw SNPs, output novel SNPs to be plotted\n",
    "    original_df.set_index(all_SNPs_column_title, inplace=True)\n",
    "    known_SNPs_df = pd.DataFrame([], index=known_SNPs)\n",
    "    to_be_removed_SNPs_df = original_df.join(known_SNPs_df, how='inner')\n",
    "    novel_df = original_df.drop(to_be_removed_SNPs_df.index.values)\n",
    "    \n",
    "    novel_df.to_csv(novel_filename)\n",
    "    return(novel_df)\n",
    "\n",
    "\n",
    "# Parameters:\n",
    "# - filename: name of input file that contains all SNPs discovered, should starts with column titles not empty line\n",
    "# - output: name of the output file to save figure of the QQ plot\n",
    "# - p_value_column_title: title of the column that contains P values in the input file\n",
    "# - title, xlabel, ylabel, dpi=300: label and resolution parameters of the figure\n",
    "# - plot_novel: a plot of novel SNPs will be plotted with original SNPs if this is set to true.\n",
    "#               User needs to supply a known SNPs file in order to look for accociated SNPs.\n",
    "# - known_SNPs_filename:\n",
    "# - novel_filename: \n",
    "# - SNPs_column_title: \n",
    "# - r2_threshold: LD threshold to query HaploReg for associated SNPs\n",
    "# - ldPop: population for LD calculation, defult is European. Other options includes AFR, AMR and ASN.\n",
    "# - plot_ci: plot shaded area of confidence interval if true\n",
    "# - ci: confidence interval, default is 0.95\n",
    "#\n",
    "# Returns: (fig, ax, lambda)\n",
    "# - fig and ax for more custermizations\n",
    "def qqplot(filename,\n",
    "           output='output.png',\n",
    "           p_value_column_title = 'P',\n",
    "           title='Q-Q plot',\n",
    "           xlabel='Expected –log10 P-values',\n",
    "           ylabel='Observed –log10 P-values',\n",
    "           dpi=300,\n",
    "           plot_novel = False,\n",
    "           known_SNPs_filename='',\n",
    "           novel_filename = 'novel_SNPs.csv',\n",
    "           known_SNPs_column_title = 'SNP',\n",
    "           all_SNPs_column_title = 'SNP',\n",
    "           r2_threshold=0.2,\n",
    "           ldPop='EUR',\n",
    "           plot_ci=True,\n",
    "           ci=0.95):\n",
    "    # Read in file, calsulate -log10 p values and lambda\n",
    "    df = __read_file(filename)\n",
    "    \n",
    "    if df is None:\n",
    "        print('Empty file, nothing is plotted')\n",
    "        return(None, None, None) # If file is empty or only has titles, return directly\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    \n",
    "    # Plot all SNPs as dots\n",
    "    Pobsv, logPobsv, logPexpt = __log_obsv_and_expt(df, p_value_column_title)\n",
    "    infl_original = __get_inflation__(Pobsv)\n",
    "    \n",
    "    ax.plot(logPexpt, logPexpt, color='r', linewidth=0.4)\n",
    "    ax.plot(logPexpt, logPobsv, linestyle='', marker='o', markersize=2, markeredgewidth=0.5,\n",
    "            fillstyle='none', color='k', zorder=3)\n",
    "    \n",
    "    # Plot shaded area of confidence intervel\n",
    "    if plot_ci==True:\n",
    "        if ci >= 1: print(\"Confidence intervel >= 1, not ci plotted\")\n",
    "        elif ci <= 0: print(\"Confidence intervel <= 0, not ci plotted\")\n",
    "        else: __plot_ci(ax, ci, logPexpt)\n",
    "    \n",
    "    infl_novel = ''\n",
    "    # Plot novel SNPs\n",
    "    if plot_novel == True:\n",
    "        if known_SNPs_filename != '':\n",
    "            # Check if known SNPs file is empty\n",
    "            if __read_file(known_SNPs_filename) is None:\n",
    "                print('Empty file of novel SNPs')\n",
    "                pass # If file is empty or only has titles, only plot original SNPs\n",
    "            else:\n",
    "                # Plot novel SNPs, then same steps as before\n",
    "                to_be_removed_SNPs = __known_SNPs(known_SNPs_filename, ldPop, r2_threshold, known_SNPs_column_title)\n",
    "                novel_SNPs_df = __remove_known_SNPs(df, to_be_removed_SNPs, all_SNPs_column_title, novel_filename)\n",
    "\n",
    "                Pobsv_novel, logPobsv_novel, logPexpt_novel = __log_obsv_and_expt(novel_SNPs_df, p_value_column_title)\n",
    "                infl_novel = __get_inflation__(Pobsv_novel)\n",
    "                \n",
    "                ax.plot(logPexpt_novel, logPobsv_novel, linestyle='', marker='o', markersize=2, markeredgewidth=0.5,\n",
    "                        fillstyle='none', color='g', zorder=1)\n",
    "                \n",
    "                annotation = \"λ (novel) = \" + str(\"{0:.4f}\".format(infl_novel))\n",
    "                ax.annotate(annotation, xy=(0.7, 0.1), xycoords='axes fraction')\n",
    "        else: print('No file of known SNPs provided\\nOnly plot original SNPs')\n",
    "\n",
    "    # Label x and y axis\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    annotation = \"λ = \" + str(\"{0:.4f}\".format(infl_original))\n",
    "    ax.annotate(annotation, xy=(0.7, 0.2), xycoords='axes fraction')\n",
    "    \n",
    "    fig.savefig(output)\n",
    "    \n",
    "    return (fig, ax, infl_original, infl_novel)  # Return for more custermizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to remove known SNPs by distance (posisiton)\n",
    "# Assume\n",
    "\n",
    "# Check files from Bommi:\n",
    "# /data100t1/home/bommi/alzheimer/metal/plot_data/ There are 3 files: ADGC, IGAP, ADGC_IGAP\n",
    "\n",
    "# known SNPs file:\n",
    "# /data100t1/home/chenh15/projects/ADGC/cond/cond.list\n",
    "# Column titles are: GENE, ens, SNP, chr, pos\n",
    "\n",
    "# Example of individul files (all SNPs)\n",
    "# /data100t1/home/bommi/alzheimer/metal/plot_data/IGAP/gtex_v7_Adipose_Subcutaneous.txt\n",
    "# Column titles are: chr, pos, P-value\n",
    "\n",
    "# Parameters needed to send to haploreg website, and some explaination\n",
    "# input_snps (query): input SNPs in a string, each SNP is separated by ','\n",
    "#        SNP number per query is set to be 1000, otherwise it takes too long and haploreg may refuse to do it.\n",
    "#        15000 SNPs did not work for test run.\n",
    "# gwas_id: the dropdown list to choose GWAS study, when no file or query SNP(s) is provided\n",
    "# r2_threshold (ldThresh): r^2 threshold, default is 0.2 in this code\n",
    "# ldPop: 1000G Phase 1 population for LD calculation. Other options includes AFR, AMR and ASN.\n",
    "# epi: Source for epigenomes\n",
    "# cons: Mammalian conservation algorithm. 'siphy'=SiPhy-omega, 'gerp'=GERP, or 'both'\n",
    "# genetypes: Show position relative to\n",
    "# output: set output result type to 'text' for python code to process\n",
    "\n",
    "# Return: everyting from Haploreg in a data frame\n",
    "def query_haploreg_all_results(input_snp,\n",
    "                   r2_threshold=0.2,\n",
    "                   ldPop='EUR',\n",
    "                   epi='vanilla',\n",
    "                   cons='siphy',\n",
    "                   genetypes='gencode'):\n",
    "\n",
    "    params_library = {'query':input_snp,\n",
    "                      'gwas_id':0,\n",
    "                      'ldThresh': r2_threshold,\n",
    "                      'ldPop': ldPop,\n",
    "                      'epi': epi,\n",
    "                      'cons': cons,\n",
    "                      'genetypes': genetypes,\n",
    "                      'output':'text'}\n",
    "    \n",
    "    # parameters passed to the website, needs to be parsed and in binary\n",
    "    params = urllib.parse.urlencode(params_library).encode(\"utf-8\")\n",
    "    # url of HaploReg4.1\n",
    "    url = 'https://pubs.broadinstitute.org/mammals/haploreg/haploreg.php'\n",
    "    # Query with parameters\n",
    "    query = urllib.request.urlopen(url, params)\n",
    "    \n",
    "    content = query.read().decode(\"utf-8\") # This is a massive string\n",
    "    content_list = content.rstrip().split('\\n')  # First element contains the line of column title\n",
    "    content_df = pd.DataFrame(columns=content_list[0].split('\\t'))\n",
    "    \n",
    "    for i in range(1, len(content_list)):\n",
    "        content_df.loc[i] = content_list[i].split('\\t')\n",
    "\n",
    "    return(content_df)\n",
    "\n",
    "\n",
    "# Tis is a funciton relmove know SNPs based on position, default is +- 10Mb\n",
    "# Get linked SNPs of given SNP\n",
    "# Parameter:\n",
    "# - snp: a single SNP to be processed. Pay attention to column titles returned from HaploReg.\n",
    "# - cutoff: default is +-10Mb from a known SNP\n",
    "# Return a dataframe version of linked SNps by position\n",
    "def get_linked_SNPs_by_pos(snp, pos, cutoff = 10000000):\n",
    "    linked_SNPs_df = query_haploreg_all_results(snp, r2_threshold=0.8)\n",
    "    linked_SNPs_df.loc[:, 'pos_hg38'] = pd.to_numeric(linked_SNPs_df.loc[:, 'pos_hg38'], errors='ignore')\n",
    "\n",
    "    # Look for LD structure +-10Mb\n",
    "    linked_SNPs_df.loc[:, 'pos_hg38'] = pd.to_numeric(linked_SNPs_df.loc[:, 'pos_hg38'], errors='ignore')\n",
    "    mask = (linked_SNPs_df.loc[:,'pos_hg38']>=(pos-cutoff))&(linked_SNPs_df.loc[:,'pos_hg38']<=(pos+cutoff))\n",
    "    remain_df = linked_SNPs_df.loc[mask]\n",
    "    \n",
    "    return(remain_df.loc[:,['chr', 'pos_hg38', 'r2','rsID','GENCODE_id','GENCODE_name','query_snp_rsid']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove known SNPs by position\n",
    "# This does not seem to work, nothing was removed for one file\n",
    "# pos: format is chrN:start-end (as required by HaploReg)\n",
    "def __remove_known_SNPs_by_pos(original_df, known_SNPs_pos,all_pos_column_title='pos_hg38',\n",
    "                        novel_filename='novel_SNPs.csv'):\n",
    "    # Keep console running\n",
    "    print('Removing known SNPs by position...')\n",
    "    \n",
    "    # Join the original dataframe and konw SNPs, output novel SNPs to be plotted\n",
    "    original_df.set_index(all_pos_column_title, inplace=True)\n",
    "    known_SNPs_df = pd.DataFrame([], index=known_SNPs_pos)\n",
    "    to_be_removed_SNPs_df = original_df.join(known_SNPs_df, how='inner')\n",
    "    novel_df = original_df.drop(to_be_removed_SNPs_df.index.values)\n",
    "    \n",
    "    content = query.read().decode(\"utf-8\")\n",
    "    # Find accociated SNPs in the content returned from HaploReg\n",
    "    matches = re.findall('rs[0-9]+', content)\n",
    "\n",
    "    # Return unique ones of the query\n",
    "#     return(list(set(matches)))\n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/data100t1/home/bommi/alzheimer/metal/plot_data/IGAP/gtex_v7_Adipose_Subcutaneous.txt'\n",
    "known_SNPs_filename = '/data100t1/home/chenh15/projects/ADGC/cond/cond.list'\n",
    "# fig, ax, infl_original, infl_novel = qqplot(file_name,\n",
    "#                                             output='2019_1004 gtex_v7_Adipose_Subcutaneous',\n",
    "#                                             p_value_column_title = 'P-value',)\n",
    "\n",
    "all_df = __read_file(file_name)\n",
    "known_df = __read_file(known_SNPs_filename)\n",
    "\n",
    "\n",
    "#  =======================\n",
    "known_SNPs_filename='/data100t1/home/chenh15/projects/ADGC/cond/cond.list'\n",
    "known_SNPs_df = __read_file(known_SNPs_filename)\n",
    "SNPs = known_SNPs_df.loc[:, 'SNP'].values\n",
    "position = known_SNPs_df.loc[:, 'pos'].values\n",
    "\n",
    "linked_SNPs_list = []\n",
    "for i in range(len(SNPs)):\n",
    "    linked_SNPs_list.append(get_linked_SNPs_by_pos(SNPs[i], position[i]))\n",
    "\n",
    "# Make a dataframe combinning all of linked SNPs, remove duplicated SNPs by position (pos_hg38)\n",
    "linked_SNPs_df = pd.concat(linked_SNPs_list)\n",
    "# return(linked_SNPs_df.drop_duplicates('pos_hg38'))\n",
    "linked_SNPs_df.drop_duplicates('pos_hg38', inplace=True)\n",
    "# linked_SNPs_df.set_index('pos_hg38', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "known_SNPs_pos = linked_SNPs_df.loc[:,'pos_hg38']\n",
    "novel_df = __remove_known_SNPs_by_pos(all_df,known_SNPs_pos, all_pos_column_title='pos')\n",
    "\n",
    "\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsIDs = query_haploreg(input_pos='chr1:207692049-207692049',\n",
    "                   r2_threshold=0.2,\n",
    "                   ldPop='EUR',\n",
    "                   epi='vanilla',\n",
    "                   cons='siphy',\n",
    "                   genetypes='gencode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query UCSC database for rsid of a SNP based on position\n",
    "# 'position':'1:207692049‘\n",
    "\n",
    "url = 'https://genome.ucsc.edu/cgi-bin/hgTables?hgsid=766133549_kgOzgBiaRp2Tx9BBIvvXK95TmfBc'\n",
    "params_library = {'position':'1:207692049',\n",
    "                 'hgsid': '766133549_kgOzgBiaRp2Tx9BBIvvXK95TmfBc',\n",
    "                  'jsh_pageVertPos': 0,\n",
    "                  'clade': 'mammal',\n",
    "                  'org': 'Human',\n",
    "                  'db': 'hg19',\n",
    "                  'hgta_group': 'varRep',\n",
    "                  'hgta_track': 'snp151',\n",
    "                  'hgta_table': 'snp151',\n",
    "                  'hgta_regionType': 'range',\n",
    "                  'position': '1:207692049',\n",
    "                  'hgta_outputType': 'primaryTable',\n",
    "                  'boolshad.sendToGalaxy': 0,\n",
    "                  'boolshad.sendToGreat': 0,\n",
    "                  'boolshad.sendToGenomeSpace': 0,\n",
    "                  'hgta_compressType': 'none',\n",
    "                  'hgta_doTopSubmit': 'get output'}\n",
    "                  \n",
    "# 'boolshad.sendToGalaxy': 0\n",
    "# 'boolshad.sendToGreat': 0\n",
    "# 'boolshad.sendToGenomeSpace': 0\n",
    "# 'hgta_outFileName': \n",
    "# 'hgta_compressType': none\n",
    "# 'hgta_doTopSubmit': get output}\n",
    "\n",
    "# =======================\n",
    "    \n",
    "# parameters passed to the website, needs to be parsed and in binary\n",
    "params = urllib.parse.urlencode(params_library).encode(\"utf-8\")\n",
    "\n",
    "# Query with parameters\n",
    "query = urllib.request.urlopen(url, params)\n",
    "    \n",
    "# # Keep console running\n",
    "# print('-- Waiting for HaploReg to return results...')\n",
    "    \n",
    "content = query.read().decode(\"utf-8\")\n",
    "\n",
    "# Find rsIDs of SNPs in the content\n",
    "matches = re.findall('rs[0-9]+', content)\n",
    "\n",
    "# Return unique ones of the query\n",
    "# return(list(set(matches)))\n",
    "\n",
    "rsIDs = set(matches)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# hgsid: 766133549_kgOzgBiaRp2Tx9BBIvvXK95TmfBc\n",
    "# jsh_pageVertPos: 0\n",
    "# clade: mammal\n",
    "# org: Human\n",
    "# db: hg19\n",
    "# hgta_group: varRep\n",
    "# hgta_track: snp151\n",
    "# hgta_table: snp151\n",
    "# hgta_regionType: range\n",
    "# position: 1:207692049\n",
    "# hgta_outputType: primaryTable\n",
    "# boolshad.sendToGalaxy: 0\n",
    "# boolshad.sendToGreat: 0\n",
    "# boolshad.sendToGenomeSpace: 0\n",
    "# hgta_outFileName: \n",
    "# hgta_compressType: none\n",
    "# hgta_doTopSubmit: get output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name2 = '/data100t1/home/bommi/alzheimer/metal/plot_data/mhannot_plot/ADGC_IGAP_genename/gtex_v7_Adipose_Subcutaneous.txt'\n",
    "df = __read_file(file_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MarkerName</th>\n",
       "      <th>chr</th>\n",
       "      <th>pos</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MTMR11</td>\n",
       "      <td>1</td>\n",
       "      <td>149900542</td>\n",
       "      <td>0.8258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EIF2D</td>\n",
       "      <td>1</td>\n",
       "      <td>206744619</td>\n",
       "      <td>0.6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZNF496</td>\n",
       "      <td>1</td>\n",
       "      <td>247460713</td>\n",
       "      <td>0.9325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNAP47</td>\n",
       "      <td>1</td>\n",
       "      <td>227916239</td>\n",
       "      <td>0.5205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATAD3C</td>\n",
       "      <td>1</td>\n",
       "      <td>1385068</td>\n",
       "      <td>0.6987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MarkerName  chr        pos  pvalue\n",
       "0     MTMR11    1  149900542  0.8258\n",
       "1      EIF2D    1  206744619  0.6900\n",
       "2     ZNF496    1  247460713  0.9325\n",
       "3     SNAP47    1  227916239  0.5205\n",
       "4     ATAD3C    1    1385068  0.6987"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
